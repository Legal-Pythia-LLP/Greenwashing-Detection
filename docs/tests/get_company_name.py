from wikirate4py import API
from pprint import pprint
import pandas as pd
from name_matching.name_matcher import NameMatcher
import time
import csv
import re
import multiprocessing

api = API("JoDaOQXoU2vzgRAbaArQlwtt")

# å¤šç·šç¨‹æŠ“ company id, company name, ISIN æ•¸é‡
PAGE_SIZE = 100
MAX_PAGES = 100000  # é ä¼°æœ€å¤šæŠ“å¹¾é ï¼Œç„¡è³‡æ–™æ™‚æœƒè‡ªå‹•åœæ­¢
OUTPUT_FILE = "wikirate_companies_all.csv"  # âœ… ä½ è¦çš„æª”å
csv_path = "wikirate_companies_all.csv"

def get_isin_count(company):
    """å¾å…¬å¸ç‰©ä»¶ä¸­è®€å– ISIN æ•¸é‡"""
    try:
        isin = getattr(company, "isin", None)
        isin_list = isin if isinstance(isin, list) else []
        return len(isin_list)
    except Exception as e:
        print(f"âš ï¸ ç„¡æ³•è™•ç†å…¬å¸ {company}: {e}")
        return 0

def worker(task_queue, result_queue, worker_id):
    """æ¯å€‹é€²ç¨‹æŠ“å–æŒ‡å®š offset é çš„è³‡æ–™"""
    while True:
        try:
            offset = task_queue.get(timeout=2)
        except:
            break  # æ²’æœ‰æ–°å·¥ä½œå°±é€€å‡º

        companies = api.get_companies(limit=PAGE_SIZE, offset=offset)
        if not companies:
            print(f"ğŸš« Worker {worker_id} - offset {offset} æ²’æœ‰è³‡æ–™ï¼Œåœæ­¢")
            break

        results = []
        for company in companies:
            isin_count = get_isin_count(company)
            results.append((str(company.id), str(company.name), isin_count))

        for row in results:
            result_queue.put(row)

        print(f"âœ… Worker {worker_id} - æŠ“å– offset {offset} å…± {len(companies)} ç­†")
        time.sleep(0.2)  # æ§åˆ¶é€Ÿåº¦é¿å… API é™åˆ¶

def parallel_fetch(num_workers=6):
    manager = multiprocessing.Manager()
    task_queue = manager.Queue()
    result_queue = manager.Queue()

    # å‹•æ…‹ç”¢ç”Ÿ offset ä»»å‹™
    for i in range(MAX_PAGES):
        task_queue.put(i * PAGE_SIZE)

    # å»ºç«‹å¤šå€‹é€²ç¨‹
    processes = []
    for i in range(num_workers):
        p = multiprocessing.Process(target=worker, args=(task_queue, result_queue, i))
        p.start()
        processes.append(p)

    # ç­‰å¾…å…¨éƒ¨å®Œæˆ
    for p in processes:
        p.join()

    print("ğŸ“ æ‰€æœ‰ worker å®Œæˆï¼Œæº–å‚™å¯«å…¥æª”æ¡ˆ...")

    # å¯«å…¥ CSV çµæœ
    with open(OUTPUT_FILE, "w", newline="", encoding="utf-8") as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(["id", "name", "isin_count"])
        while not result_queue.empty():
            writer.writerow(result_queue.get())

    print(f"ğŸ“ å·²å¯«å…¥ {OUTPUT_FILE}")

# âœ… åŸ·è¡Œä¸»ç¨‹å¼
if __name__ == "__main__":
    parallel_fetch(num_workers=6)

# ============================================================================================================

# åå­—æ¨¡ç³Šæ¯”å°
# âœ… è‡ªè¨‚ normalization æ–¹æ³•ï¼ˆæ¨¡ä»¿ NameMatcher transform=Trueï¼‰
def normalize_name(name: str) -> str:
    name = name.lower()
    name = re.sub(r'[^a-z0-9\s]', '', name)   # ç§»é™¤æ¨™é»ç¬¦è™Ÿ
    name = re.sub(r'\s+', ' ', name)          # ç§»é™¤å¤šé¤˜ç©ºç™½
    return name.strip()


# âœ… ä¸»å‡½æ•¸ï¼šæ ¹æ“šè¼¸å…¥åç¨±æ¨¡ç³Šæ¯”å°ï¼Œä¸¦æ ¹æ“š ISIN æ•¸é‡é¸æ“‡æœ€ä½³åŒ¹é…
def find_best_matching_company(input_name: str, wikirate_companies: list) -> str:
    
    keyword = input_name.lower()
    filtered_companies = [c for c in wikirate_companies if keyword in c['name'].lower()]
    if not filtered_companies:
        print("âŒ æ‰¾ä¸åˆ°ä»»ä½•åç¨±åŒ…å«é—œéµå­—çš„å…¬å¸")
        return None

    # âœ… å°å‡ºæ‰€æœ‰ç¬¦åˆæ¢ä»¶çš„å…¬å¸åç¨±
    print("ğŸ” æ‰¾åˆ°ä»¥ä¸‹åŒ…å«é—œéµå­—çš„å…¬å¸ï¼š")
    for c in filtered_companies:
        print(f" - {c['name']}")

    company_names = [c['name'] for c in filtered_companies]

    # å»ºç«‹è½‰æ›å°ç…§è¡¨
    normalized_map = {}
    for c in wikirate_companies:
        original_name = c['name'] if isinstance(c, dict) else c
        normalized = normalize_name(original_name)
        normalized_map[normalized] = {
            'original_name': original_name,
            'isin_count': c.get('isin_count', 0) if isinstance(c, dict) else 0
        }

    df_master = pd.DataFrame({'Company name': company_names})
    df_input = pd.DataFrame({'name': [input_name]})

    matcher = NameMatcher(
        number_of_matches=5,
        legal_suffixes=True,
        common_words=False,
        top_n=50,
        verbose=False
    )
    matcher.set_distance_metrics(['bag', 'typo', 'refined_soundex'])
    matcher.load_and_process_master_data(column='Company name', df_matching_data=df_master, transform=True)
    matches = matcher.match_names(to_be_matched=df_input, column_matching='name')

    if matches.empty:
        return None

    # ğŸ§ª å°å‡ºæ‰€æœ‰åŒ¹é…çš„åç¨±èˆ‡åˆ†æ•¸
    print("ğŸ§ª æ‰€æœ‰åŒ¹é…çµæœï¼š")
    results = []
    for i in range(5):
        match_name_col = f'match_name_{i}'
        score_col = f'score_{i}'
        if match_name_col in matches.columns and score_col in matches.columns:
            match_name = matches.at[0, match_name_col]
            score = matches.at[0, score_col]
            if pd.notna(match_name):
                normalized = normalize_name(match_name)
                isin_count = normalized_map.get(normalized, {}).get('isin_count', 0)
                print(f"{i+1}. {match_name}  ğŸ‘‰ åˆ†æ•¸: {score:.2f}  ğŸ†” ISINæ•¸é‡: {isin_count}")
                results.append((normalized, score))

    if not results:
        return None

    # æ‰¾å‡ºæœ€é«˜åˆ†
    max_score = max(score for _, score in results)
    top_matches = [name for name, score in results if score == max_score]

    # å¦‚æœåªæœ‰ä¸€å€‹æœ€é«˜åˆ† â†’ å›å‚³åŸå§‹åç¨±
    if len(top_matches) == 1:
        return normalized_map.get(top_matches[0], {}).get('original_name', top_matches[0])

    # å¦‚æœæœ‰å¤šå€‹æœ€é«˜åˆ† â†’ ç”¨ isin_count æŒ‘é¸
    best_match = max(top_matches, key=lambda name: normalized_map.get(name, {}).get('isin_count', 0))
    return normalized_map.get(best_match, {}).get('original_name', best_match)




